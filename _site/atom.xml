<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>alelouis</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2017-11-14T23:35:28+01:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Alexis LOUIS</name>
   <email>wimacod@gmail.com</email>
 </author>

 
 <entry>
   <title>Latent Space Interpolations</title>
   <link href="http://localhost:4000/2017/11/12/latent-space-interpolations/"/>
   <updated>2017-11-12T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/11/12/latent-space-interpolations</id>
   <content type="html">
&lt;p&gt;My intent with this post is to formalize thoughts I came across while playing with generative models and especially VAEs. Firstly interested by visual aspects of latent space interpolation, I found myself digging deeper to gain high-dimensionnal intuitions and share here my understanding.&lt;/p&gt;

&lt;p&gt;The toy we’ll be playing with is a simple VAE. I will start by making a brief summary of AE and VAE principles and then move to latent space interpolations (where beautiful things happen).&lt;/p&gt;

&lt;h2 id=&quot;auto-encoders&quot;&gt;Auto-Encoders&lt;/h2&gt;

&lt;p&gt;Auto-encoders are unsupervised neural networks architectures used to encode data into &lt;strong&gt;structured representations&lt;/strong&gt;. While they can be used for dimensionality reduction (instead of PCA or t-SNE for example) various other applications are being explored (anomaly detection, generative models, exploratory analysis…).&lt;/p&gt;

&lt;p&gt;Unlike typical supervised tasks, an auto-encoder learns an identity function between its inputs and outputs. While in the former a neural network typically learns a mapping between input samples and output labels, the latter is optimized to reconstruct the input the best it can.&lt;/p&gt;

&lt;p&gt;This objective is not inherently useful, we are more interested in what lies in the hidden layers, or for this example the middle layer &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt;. We call &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; the &lt;strong&gt;encoded&lt;/strong&gt; or &lt;strong&gt;latent&lt;/strong&gt; vector. The left part of the graph is the encoder part, it encodes the input into the latent space. The latent vector is then decoded by the decoder and outputs the input reconstruction. By setting the length of the middle vector &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; to a smaller value than the input dimension, the mapping has to find an efficient way to compress input samples into a smaller representations. The mapping (set of neural weights and biases) is learned during training phase by minimizing the reconstruction error. The loss is often chosen between MSE (Mean Squared Error) and cross-entropy.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;../images/ae.svg&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Usage example&lt;/em&gt; : one wants to encode images of digits from their &lt;em&gt;28x28&lt;/em&gt; pixels 2D representation to a more compact representation, i.e. a vector of length 2. During training phase, many samples are proposed to the network with the reconstruction error between the original image and the reconstructed output image being minimized. In the middle of the network is specified an hidden layer - the 2-dim vector &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; - so that all information &lt;em&gt;has&lt;/em&gt; to be compressed into 2 values at some point. Going from 784 dimensions to 2 is quite a drastic reduction, it forces the network to converge to efficient compression solutions. What we hope here is that the low-dimensional representation will learn in an unsupervised way underlying concepts about the data (there are 10 digits, some orientation, thickness…) and encode each one of this concepts into a dimension of the latent space.&lt;/p&gt;

&lt;p&gt;It makes sense, but the auto-encoder &lt;em&gt;can&lt;/em&gt; cheat.&lt;/p&gt;

&lt;p&gt;Suppose the dimension of &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; is slightly higher. What mecanism can ensure that the learned representation is not a trivial mapping between far apart regions of latent space and all input samples. Well, nothing. For example, the latent vector could easily encode each sample into binary positions (0001, 0010, 0011…) and have excellent reconstruction performance while having learn &lt;strong&gt;nothing&lt;/strong&gt; about data structure.&lt;/p&gt;

&lt;p&gt;Because the network finds its own ways to compress data, we can’t understand fully its representation by looking at it. Nothing guarantees that the vector &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z_1} = [2.2,1.3]&lt;/script&gt; is decoded as an image of a 3 and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z_2} = [2.1,1.4]&lt;/script&gt;, while very close in latent space, is decoded as an image &lt;em&gt;also&lt;/em&gt; looking like a 3. Any interpretation of what latent dimensions represent is here arduous.&lt;/p&gt;

&lt;p&gt;Moreover, its hard to generate samples from this type of network. In order to generate a sample you have to find a latent vector &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; that will produce, once decoded, a plausible output (in the sense similar to inputs on which the AE was trained). The manifold represented by all the latent vectors associated to input samples is probably not covering the entire &lt;script type=&quot;math/tex&quot;&gt;\textbf{z}&lt;/script&gt; space and sampling randomly will surely yields unprobable results.&lt;/p&gt;

&lt;p&gt;These limitations, latent space interpretation and generation, are trying to be solved with VAEs.&lt;/p&gt;

&lt;h2 id=&quot;variational-auto-encoders&quot;&gt;Variational Auto-Encoders&lt;/h2&gt;

&lt;p&gt;I was lately interesed in VAEs or Variational Auto-Encoder. This name is actually a contraction of two fields not typically associated : variational inference and deep learning auto-encoders. Here is my attempt to explain VAEs basic necessary principles to understand latent space exploration (a bit of math but nothing too nerdy).&lt;/p&gt;

&lt;p&gt;A VAE is an auto-encoder with stochastic components. The main motivation behind this design is to provide generative capabilities to the model by constraining the latent space shape.&lt;/p&gt;

&lt;p&gt;The nice feature we want is to be able to sample latent vectors from well known distributions and get from the decoding network plausible samples. VAEs precisely to that : in addition to regular AE behaviour i.e. identity reconstruction, we constrain the latent space with a specified distribution, or prior (often noted as &lt;script type=&quot;math/tex&quot;&gt;P(z)&lt;/script&gt;). The new loss function now has to penalize &lt;strong&gt;bad reconstruction&lt;/strong&gt; performance (same as in AEs) and &lt;strong&gt;unconstrained latent representation&lt;/strong&gt;. To make sure the latent representation is well contrained by our prior we minimize a distance between actual latent space distribution &lt;script type=&quot;math/tex&quot;&gt;Q(z\mid x)&lt;/script&gt; and our chosen prior &lt;script type=&quot;math/tex&quot;&gt;P(z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;An easy to sample from and simple prior we use is the unit centered multivariate normal.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(z) \sim \mathcal{N}(\textbf{0}, I)&lt;/script&gt;

&lt;p&gt;KL-divergence is often used to compare two distribution. Considering all elements, our final loss can then be expressed as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} =  -E_{z\sim Q(z\mid x)}[log(p(x\mid z))] + KL(Q(z\mid x)\mid \mid P(z))&lt;/script&gt;

&lt;p&gt;We then take the assumption that the posterior is following an isotropic Gaussian distribution to simplify the KL divergence calculus (&lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt; has dimension &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;). If you want details you can check the demonstration in the &lt;a href=&quot;[https://arxiv.org/abs/1312.6114]&quot;&gt;original paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It also has the avantage to simplify our architecture : the encoder network only encodes two vectors for each sample : one of means and one of variances (the Gaussian components are supposed independent hence the need for only one variance &lt;strong&gt;vector&lt;/strong&gt;). Many realizations of latent vectors can then sampled from the multivariate normal distribution. The training phase ensures that the posterior &lt;script type=&quot;math/tex&quot;&gt;Q(z\mid x)&lt;/script&gt; doesn’t get too far from the prior &lt;script type=&quot;math/tex&quot;&gt;P(z)&lt;/script&gt; while also giving good reconstruction performance.&lt;/p&gt;

&lt;p&gt;I personnally consider I don’t fully understand a concept if I can’t make it work in practice. See this simple VAE implementation in PyTorch at &lt;a href=&quot;https://github.com/alelouis/vae&quot;&gt;github.com/alelouis/vae&lt;/a&gt;. I recommand playing with it and reimplementing it as an exercise.&lt;/p&gt;

&lt;p&gt;By running the network on MNIST with a latent space of dimension 2 we can explore the latent space manifold, a very visual part.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;latent-space-interpolations&quot;&gt;Latent space interpolations&lt;/h2&gt;

&lt;p&gt;Latent variables are &lt;em&gt;not observed&lt;/em&gt; variables that are describing the underlying properties of a process. For example, the vector of means &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; of an observed sequence of observations &lt;script type=&quot;math/tex&quot;&gt;x \sim \mathcal{N}(\theta, \sigma)&lt;/script&gt; is a latent variable. We only get to see &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; while hidden &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; describe the underlying generating process from which observations are sampled.&lt;/p&gt;

&lt;p&gt;The mean of a Gaussian distribution is not a captivating latent variable (compared to what we &lt;em&gt;can&lt;/em&gt; find). Last years have seen a growing hype for latent variable generative models such as variational auto-encoders. A large part of this enthusiasm is due to their ability to &lt;em&gt;infer&lt;/em&gt; latent variables, real explanatory variables, for a large scope of problems. Moreover, found latent variable &lt;a href=&quot;https://docs.google.com/presentation/d/12uZQ_Vbvt3tzQYhWR3BexqOzbZ-8AeT_jZjuuYjPJiY/pub?start=true&amp;amp;loop=true&amp;amp;delayms=30000#slide=id.g1329951dde_0_0&quot;&gt;can be constrained&lt;/a&gt; to be relevant and we, &lt;em&gt;humans&lt;/em&gt;, can sometimes interpret them.&lt;/p&gt;

&lt;p&gt;We can use visual tools to explore latent space and understand what the model is doing.&lt;/p&gt;

&lt;h3 id=&quot;2d-latent-space-on-mnist-vae&quot;&gt;2D Latent Space on MNIST VAE&lt;/h3&gt;

&lt;p&gt;Humans are very limited (yes), one reason why is that we can’t visually conceptualize objects in more than 3 dimensions. The good news is that we have a tool to understand hidden behavior even if we can’t &lt;em&gt;see&lt;/em&gt; it, it’s called &lt;strong&gt;maths&lt;/strong&gt; (sorry). But for now let’s stick to our comfort zone. We will sneak peek into a 2-dimensional latent space from a VAE trained on MNIST samples. As explained previously, we can generate latent vector by sampling from our prior &lt;script type=&quot;math/tex&quot;&gt;P(z) \sim \mathcal{N}(\textbf{0}, I)&lt;/script&gt;. Sampling from this distribution gives us random samples belonging to the 2D plane with many samples being generated near the mean &lt;script type=&quot;math/tex&quot;&gt;[0,0]&lt;/script&gt;. If we want to explore the latent space smoothly we can ignore Gaussian sampling for a moment and follow a continuous path on the 2D plane where our prior has high probability. The first idea we can get is to decode latent vectors belonging to a centered grid in latent space. In Python / PyTorch we could decode each grid point (latent vector) and display each output and the 2D plane.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;final_img_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_pixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_pixel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_pixel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pixel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;y_pixel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x_pixel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_pixel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../images/latent_space.png&quot; alt=&quot;dim&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Beautiful isn’t it ? The most fascinating thing is the smooth transitions between each of our human concepts (digits). Even if we have only two dimension, we can already see glimpses of the learned structure. The first dimension of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt; encodes a bit the orientation attribute. See how generated images in the negative &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z_1}&lt;/script&gt;values are tilted left, while positive values are tilted right. By setting intentionnally 2 latent dimensions thee network has to encode many concepts within the same dimensions introducing correlated attributes.&lt;/p&gt;

&lt;p&gt;The grid sampling is pretty but static, how is it like to &lt;em&gt;move&lt;/em&gt; inside this space ?&lt;/p&gt;

&lt;p&gt;We can set up a simple parametric curve describing a spiral inside the domain &lt;script type=&quot;math/tex&quot;&gt;x, y \in [-2,2]&lt;/script&gt; and decode at regular interval (in parametric space) the latent vector to output the image.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{lcl} x(t) &amp; = &amp; (0.1\cdot t\cdot 2 - 2)\cdot \sin(t) \\ y(t) &amp; = &amp; (0.1\cdot t\cdot 2 - 2)\cdot \cos(t) \\ t &amp; \in &amp; [0,3\pi]\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;We visualize the trace of &lt;script type=&quot;math/tex&quot;&gt;(x(t), y(t))&lt;/script&gt; overlayed on the Gaussian prior, the latent vector components at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and the corresponding decoded image.&lt;/p&gt;

&lt;div style=&quot;text-align:center;&quot;&gt;
&lt;video style=&quot;margin: 0 auto; width: 100%; max-width: 1020px;&quot; autoplay=&quot;&quot; loop=&quot;loop&quot;&gt;

```
   &lt;source src=&quot;../images/animation.mp4&quot; type=&quot;video/mp4&quot; /&gt;
```

   &lt;/video&gt;
&lt;/div&gt;

&lt;h3 id=&quot;what-about-higher-dimensions-&quot;&gt;What about higher dimensions ?&lt;/h3&gt;

&lt;p&gt;Going up in dimensions for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt; is fairly easy in code, but we have to be careful when exploring high-dimensional spaces. After seeing the 2D grid, it’s easy to think about a 3D box. Now think about a 100D box. Hard right ? Not only it’s impossible for us to have a visual representation of such objects, but our intuition in low dimensional spaces are just wrong as we go up in dimensions.&lt;/p&gt;

&lt;p&gt;To demonstrate how bad our intuitions can be let’s think about a simple object : all its forming points are at the same distance of another point, called the center. In &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^2}&lt;/script&gt; it corresponds to a circle, in &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^3}&lt;/script&gt; a sphere … Now let’s compute a basic property of this object :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Area of the unit-circle : &lt;script type=&quot;math/tex&quot;&gt;V_2 = \pi&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Volume of unit-sphere : &lt;script type=&quot;math/tex&quot;&gt;V_3 = \frac{4}{3}\pi​&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Volume of unit 4-sphere : &lt;script type=&quot;math/tex&quot;&gt;V_4 = \frac{1}{2}\pi^{2}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The volume seems to increase, nothing strange.
Funny things happen when considering the general expression :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_n={\pi^{n/2}\over \Gamma(n/2+1)}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;../images/sphere.svg&quot; alt=&quot;dim&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Oops. This is something our intuition has struggles dealing with. Now, this surely has consequences in high-dimensionnal space exploration. In fact, because the volume of hyperspheres goes to 0 as dimensions increase, Gaussian sampling is a bit &lt;em&gt;different&lt;/em&gt;. While a multivariate normal still has its maximum at the origin, if we were to compute the probability that a sample belongs to a given domain, like a unit &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-sphere centered at origin, we would integrate the density on the hypersphere volume (an hyperball ? puns.), which is very close to 0 for large &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;. This mean we have a very low probability of having samples close to origin (which is not the case in low-dimensions). More precisely, for the very reason we have to increase the radius of the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-sphere significantly for it to have a sufficient volume and therefore a bit of Gaussian integrated density, there is a gap with almost zero probabilty. All sampling is concentrated within a slice of the space. We can check this phenomenon by plotting the norm of sampled vectors for multivariate normal of various dimensions :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/dim.svg&quot; alt=&quot;dim&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is what is called the &lt;strong&gt;norm concentration&lt;/strong&gt; : for large dimensions &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; all the samples have their &lt;script type=&quot;math/tex&quot;&gt;L^{p}&lt;/script&gt; norms concentrated in an annulus of radius &lt;script type=&quot;math/tex&quot;&gt;\sqrt{p}&lt;/script&gt;. This is very similar to what a uniform spherical distribution would give us. This has serious implication when latent space interpolation is used in high dimensions. As we were wandering around within a grid in 2D space or a box in 3D, we were actually &lt;em&gt;covering&lt;/em&gt; the prior distribution used by our model. With Gaussians behaving like spherical uniform distributions in high dimensions, we have to make sure we are exploring space &lt;em&gt;within&lt;/em&gt; high probability spaces. One way is to only decode latent vector having an &lt;script type=&quot;math/tex&quot;&gt;L^{p}&lt;/script&gt; norm of &lt;script type=&quot;math/tex&quot;&gt;\sqrt{p}&lt;/script&gt; (or belonging to a &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;-sphere of radius &lt;script type=&quot;math/tex&quot;&gt;\sqrt{p}&lt;/script&gt;). Said differently we can move on the surface of an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-sphere. The use of spherical coordinates is well suited here. Everyone is well aware of 2 (maybe 3) dimensions equations systems, but here is the &lt;script type=&quot;math/tex&quot;&gt;n &gt; 3&lt;/script&gt; version :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
x_1 &amp;= r \cos(\theta_1) \\
    &amp;\vdots\\
x_{n-1} &amp;= r \sin(\theta_1) \cdots \sin(\theta_{n-2}) \cos(\theta_{n-1}) \\
x_n &amp;= r \sin(\theta_1) \cdots \sin(\theta_{n-2}) \sin(\theta_{n-1})\\
\theta_{1...n-2} &amp;\in [0,\pi]\\
\theta_{n-1} &amp;\in [0,2\pi]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For instance, the surface of a &lt;script type=&quot;math/tex&quot;&gt;5&lt;/script&gt;-sphere of radius &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; can be parameterized with these 5 cartesian coordinates
(the color coding shows how I remember the structure):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\definecolor{r}{RGB}{18,110,213}
\definecolor{sin}{RGB}{217,70,70}
\definecolor{cos}{RGB}{10,150,40}


x_1 &amp;= \color{r} r \color{cos} \cos(\theta_1) \\
x_2 &amp;= \color{r} r \color{sin} \sin(\theta_1)\color{cos} \cos(\theta_2) \\
x_3 &amp;= \color{r} r \color{sin} \sin(\theta_1)\sin(\theta_{2})\color{cos}\cos(\theta_{3}) \\
x_4 &amp;= \color{r} r \color{sin} \sin(\theta_1)\sin(\theta_{2}) \sin(\theta_{3})\color{cos}\cos(\theta_{4})\\
x_5 &amp;= \color{r} r \color{sin} \sin(\theta_1)\sin(\theta_{2}) \sin(\theta_{3})\sin(\theta_{4})
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;I can’t really figure out what movement theses equations are actually doing in &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^n}&lt;/script&gt; spaces, what I can do however is plot the &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^3}&lt;/script&gt; system. By setting all &lt;script type=&quot;math/tex&quot;&gt;\theta_{1...n-2}&lt;/script&gt; between &lt;script type=&quot;math/tex&quot;&gt;[0,\pi]&lt;/script&gt; &lt;strong&gt;with&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;\theta_{n-1}&lt;/script&gt; covering a multiple of &lt;script type=&quot;math/tex&quot;&gt;2\pi&lt;/script&gt; I can explore the sphere in a continuous spiraling effect. The blue line is the 3D version path we will likely take to explore high dimensional &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt; spaces while all blue scatter points are potential realization of a spherical uniform distribution (approximation to what an &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^n}&lt;/script&gt; Gaussian really is).&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;margin: 0 auto; display: block; width : 75%;&quot; src=&quot;../images/sphere_3d.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I trained another VAE with this time 10 dimensions for &lt;script type=&quot;math/tex&quot;&gt;\mathbf{z}&lt;/script&gt;. Because I can’t plot the path taken, I want to be sure I cover a large part of the &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-sphere surface as I did in the &lt;script type=&quot;math/tex&quot;&gt;{\rm I\!R^3}&lt;/script&gt; version by making many loops around the ball. To really understand what the equations are describing I find it usefull to look the 3 projections of the previous path. It’s easy to see how &lt;script type=&quot;math/tex&quot;&gt;\theta_{2}&lt;/script&gt; is parameterizing a circle (&lt;script type=&quot;math/tex&quot;&gt;\cos(\theta_{2})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sin(\theta_{2})&lt;/script&gt;) which is scaled by the &lt;script type=&quot;math/tex&quot;&gt;\sin&lt;/script&gt; sequence preceding it. The two other planes simply cover a semi-circle modulated on one component by the spiral amplitude. Simiar things happens in higher dimensions, with only the last two &lt;script type=&quot;math/tex&quot;&gt;x_{n}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_{n-1}&lt;/script&gt; describing a spiral (which is why we will make &lt;script type=&quot;math/tex&quot;&gt;\theta_{n-1}&lt;/script&gt; cover of large multiple of &lt;script type=&quot;math/tex&quot;&gt;2\pi&lt;/script&gt;) and all the other planes covering the &lt;script type=&quot;math/tex&quot;&gt;n-1&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;[0,\pi]&lt;/script&gt; intervals.&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;margin: 0 auto; display: block; width : 100%;&quot; src=&quot;../images/slices.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here are the 6 slices of a &lt;script type=&quot;math/tex&quot;&gt;4&lt;/script&gt;-sphere :&lt;/p&gt;

&lt;p&gt;&lt;img style=&quot;margin: 0 auto; display: block; width : 75%;&quot; src=&quot;../images/4d.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To increase the length of the interpolation animation we can explore a denser mesh on the hypersphere surface. The interval covered by &lt;script type=&quot;math/tex&quot;&gt;\theta_{n-1}&lt;/script&gt; determines the number of loops done. A period of &lt;script type=&quot;math/tex&quot;&gt;2\pi&lt;/script&gt; means that the start point is reached at the end point. A period of &lt;script type=&quot;math/tex&quot;&gt;2n\pi&lt;/script&gt; means the start point is reached &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; time forming &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; loops. Therefore, the mesh density can be tweaked with the number of &lt;script type=&quot;math/tex&quot;&gt;\theta_{n-1}&lt;/script&gt; periods done while &lt;script type=&quot;math/tex&quot;&gt;\theta_{1...n-2}&lt;/script&gt; go through &lt;script type=&quot;math/tex&quot;&gt;[0,\pi]&lt;/script&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pca</title>
   <link href="http://localhost:4000/2017/10/14/pca/"/>
   <updated>2017-10-14T00:00:00+02:00</updated>
   <id>http://localhost:4000/2017/10/14/pca</id>
   <content type="html">&lt;h1 id=&quot;intuitive-explanations-of-pca-underused-concepts&quot;&gt;Intuitive explanations of PCA underused concepts&lt;/h1&gt;

&lt;p&gt;Work heavily in progress&lt;/p&gt;

&lt;p&gt;This guide is intended to be a simple, non-exhaustive but straight-forward explanation of principal component analysis technique applied to data analysis.
Code and graphs are in R language.
https://www.r-project.org/
PCA is widely known as a &lt;strong&gt;dimension reduction&lt;/strong&gt; technique and less for its &lt;strong&gt;exploratory data analysis&lt;/strong&gt; tools.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;Your data should describe a given number of observations / individuals under many features / variables.
This is typically represented as a n x m matrix with n individuals and m variables.
PCA can be applied to your dataset using many R functions. I’ll use PCA from package FactoMinerR.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;res.pca &amp;lt;- PCA(dataset)
res.pca
name               description                          
1  &quot;$eig&quot;             &quot;eigenvalues&quot;                        
2  &quot;$var&quot;             &quot;results for the variables&quot;          
3  &quot;$var$coord&quot;       &quot;coord. for the variables&quot;           
4  &quot;$var$cor&quot;         &quot;correlations variables - dimensions&quot;
5  &quot;$var$cos2&quot;        &quot;cos2 for the variables&quot;             
6  &quot;$var$contrib&quot;     &quot;contributions of the variables&quot;     
7  &quot;$ind&quot;             &quot;results for the individuals&quot;        
8  &quot;$ind$coord&quot;       &quot;coord. for the individuals&quot;         
9  &quot;$ind$cos2&quot;        &quot;cos2 for the individuals&quot;           
10 &quot;$ind$contrib&quot;     &quot;contributions of the individuals&quot;   
11 &quot;$call&quot;            &quot;summary statistics&quot;                 
12 &quot;$call$centre&quot;     &quot;mean of the variables&quot;              
13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot;    
14 &quot;$call$row.w&quot;      &quot;weights for the individuals&quot;        
15 &quot;$call$col.w&quot;      &quot;weights for the variables&quot;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;output&quot;&gt;Output&lt;/h2&gt;

&lt;p&gt;The result of the function PCA contains a lot but similar information.
Most of people are familiar with PCA concepts and the fact it vaguely project multidimensional data into fewer principal components or factors were inertia / variance / information is maximized. PCA use is often sadly reduced to “project my 86 dimensional dataset into 2 so I can see a damn thing about what I’m doing”. This guide makes the assumption you are familiar with PCA general principles but want to exploit more information from it.&lt;/p&gt;

&lt;p&gt;Here we tackle the non so-obvious parameters returned by PCA function coord, cor, cos and contrib.
A quick research of these terms will lead you to complex stack overflow / documentation of mind-blowing facts about eigenvalues, loadings, cosines between vectors standardized (or not) projected (or not). The internet polysemy is inherent to the “apparent” existing confusion and the difficulty of finding directly exploitable information about theses PCA outputs is abnormally high.&lt;/p&gt;

&lt;p&gt;This guide is for those who want to drive meaningful interpretation based on PCA results without losing time crossing theory aspects and algorithms implementations.&lt;/p&gt;

&lt;h2 id=&quot;variables-or-individuals&quot;&gt;Variables or individuals&lt;/h2&gt;

&lt;p&gt;The fun thing about PCA (and its friends MCA, FAMD etc…) is that you can either consider variables or individuals as your input vector space. In a traditional way, if we consider individuals as our rows, each individuals is represented by a point of M dimensions (one for each variable). On the other hand, by transposing the matrix we can see variables points of n individuals components. Keep that in mind for further analysis.&lt;/p&gt;

&lt;h3 id=&quot;individuals&quot;&gt;Individuals&lt;/h3&gt;
&lt;p&gt;We’ll explore the PCA outputs related to individuals (the most common way people think about PCA).&lt;/p&gt;
&lt;h5 id=&quot;indcoord&quot;&gt;$ind$coord$&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	Dim.1        Dim.2        Dim.3        Dim.4        Dim.5        Dim.6       Dim.7       Dim.8
1   1.87527565 -2.118830852 -0.523052270 -1.498565045  1.241140053  0.096625894  0.10420419  0.51855570
2   0.28286233  0.709260347 -0.037959700 -0.799378852  0.024049897 -0.027015764 -0.15126017  0.00663232
3   0.01049434  1.522753419 -0.685525022  0.540867877 -0.620855870  0.770635782 -0.76437671  0.02248488
4   0.73931610  0.758334963 -1.404787083  0.531440029 -0.589536837  0.092011347  0.81687900 -0.37774997
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Each individual originally in the m-dimensional feature space is projected on the principal components or factors. Theses are simply the coordinates of the projection. Remember components are sorted by explained inertia.&lt;/p&gt;

&lt;h5 id=&quot;indcos2&quot;&gt;$ind$cos2&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	Dim.1        Dim.2        Dim.3        Dim.4        Dim.5        Dim.6        Dim.7        Dim.8
1  2.483061e-01 3.169931e-01 1.931734e-02 1.585654e-01 1.087674e-01 6.592411e-04 7.667038e-04 1.898664e-02
2  3.637151e-02 2.286770e-01 6.550232e-04 2.904801e-01 2.629284e-04 3.317765e-04 1.040065e-02 1.999595e-05
3  2.317072e-05 4.878523e-01 9.887257e-02 6.154766e-02 8.109812e-02 1.249475e-01 1.229261e-01 1.063679e-04
4  1.033726e-01 1.087595e-01 3.732211e-01 5.341383e-02 6.573052e-02 1.601135e-03 1.262004e-01 2.698697e-02
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;cos2 (pronounced cos squared) is often defined as measure of the quality of representation. It’s possible to gain a visual intuition on why that is true. First things first, what cosine is here squared ? It’s the cosine between the original vector representing our individual and the corresponding projected vector on principal components. This angle from simple geometry formulas is the ratio of the norm of the projected vector by the norm of the original vector. Albeit cos2 is the squared version of the latter. Squaring it has the nice propriety of making the sum of all cos2 for a given individual equal 1 (considering all principal components).&lt;/p&gt;

&lt;p&gt;You should still wonder why this has anything to do with a quality of representation. Here’s how you can understand it : the original individual vector in our m-dimensional space is projected on a few principal components, if the original vector is in a similar direction as the principal component then it successfully captures the direction of this individual vector and the projection will be a good representation of the original vector information. In this case the angle between them is small or near pi i.e cosine near 1 / -1 -&amp;gt; cosine squared near 1. In the other case if the angle between the two is larger (there is a part of the original vector information that is not projected on the principal component) it means that the original vector has a much worst projection (cos2 is less than 1).&lt;/p&gt;

&lt;p&gt;The nice thing about PCA is that principal components are built orthogonally, so if you want to compute the quality of representation cos2 for the first components plane (how well is my individual represented by the first two principal components) you can simple add the cos2 of dimension 1 and dimension 2. You should now understand why it made sense to have the sum of all cos2 across all components equal 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpretation facts&lt;/strong&gt; : projected vectors with low cos2 have a bad representation in principal components. In consequence you should not base your whole interpretation on these individuals.&lt;/p&gt;

&lt;h5 id=&quot;indcontrib&quot;&gt;$ind$contrib&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	Dim.1        Dim.2        Dim.3        Dim.4        Dim.5        Dim.6        Dim.7        Dim.8
1  2.483061e-01 3.169931e-01 1.931734e-02 1.585654e-01 1.087674e-01 6.592411e-04 7.667038e-04 1.898664e-02
2  3.637151e-02 2.286770e-01 6.550232e-04 2.904801e-01 2.629284e-04 3.317765e-04 1.040065e-02 1.999595e-05
3  2.317072e-05 4.878523e-01 9.887257e-02 6.154766e-02 8.109812e-02 1.249475e-01 1.229261e-01 1.063679e-04
4  1.033726e-01 1.087595e-01 3.732211e-01 5.341383e-02 6.573052e-02 1.601135e-03 1.262004e-01 2.698697e-02
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WIP&lt;/p&gt;

&lt;h3 id=&quot;variables&quot;&gt;Variables&lt;/h3&gt;
&lt;p&gt;Here we start to tackle a non so-intuitive way of seeing variables outputs.&lt;/p&gt;
&lt;h5 id=&quot;varcoord&quot;&gt;$var$coord&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;				  		Dim.1        Dim.2       Dim.3       Dim.4      Dim.5       Dim.6        Dim.7
Block.size.median         0.58312195 -0.436142784  0.05417665 -0.28264571  0.1487591  0.49036623 -0.229737181
Altitude                  0.72890298 -0.153895038  0.47250463 -0.03153766  0.1581047 -0.14076494 -0.169718120
p20                       0.46360753 -0.167851672  0.42165395 -0.46834161 -0.3038508 -0.12564113  0.483601604
Wetness.index            -0.36685535  0.670561706  0.32344711 -0.35569038 -0.0187711  0.16681691  0.001830206
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The first thing you should ask yourself is what could possibly represent the coordinates of a variable.
A variable vector is not obviously not like an individual. A variable vector should be centered (having a mean of 0) and is probably reduced if you have different units (standard deviation equal to 1). So all variables can be seen as unit-scaled vectors all “touching” an hypersphere of radius 1. The coord returned by the PCA function represent correlations between the unit-scaled variable vectors in respect to principal components. So for example the variable Block.size.median has a positive correlation coefficient 0.58 with first principal component and negative correlation -0.43 with second component. The expression of the correlation coefficient between a variable and a principal component here is exactly the same as to expression of the cosine angle between them (that’s a nice property).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpretation facts&lt;/strong&gt; : The most common way to plot variable output of PCA is by representing each variable in a plane with their coordinates (being correlations coefficients) and a unit circle (which has the fancy name of Circle of Correlations). This visually gives you information whether a variable is increasing or decreasing in respect to principal components axis. On the below example we see that the variable altitude is correlated with first principal components meaning individual on the right side of the plane will have high altitude values. In a similar fashion individuals on the top side would have high Wetness.index values (positively correlated).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;cor.png&quot; alt=&quot;Correlation circle&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;varcos2&quot;&gt;$var$cos2&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;						 Dim.1        Dim.2        Dim.3        Dim.4        Dim.5       Dim.6        Dim.7
Block.size.median        0.340031208 1.902205e-01 0.0029351095 0.0798885977 0.0221292660 0.240459044 5.277917e-02
Altitude                 0.531299549 2.368368e-02 0.2232606257 0.0009946243 0.0249971082 0.019814767 2.880424e-02
p20                      0.214931941 2.817418e-02 0.1777920525 0.2193438629 0.0923253350 0.015785693 2.338705e-01
Wetness.index            0.134582846 4.496530e-01 0.1046180310 0.1265156449 0.0003523541 0.027827880 3.349653e-06
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the case of individuals cos2 was the square of the cosine between the observation and the principal component. For variables that doesn’t change : cos2 is the square of the cosine between to variable original vector and the principal component. The nice thing is that we already have the cosine between the vector variable and the principal component : it’s the coordinates, cos2 calculation is then trivial. The even nicer thing is that the underlying geometry being the same, the interpretation about quality of representation is the same as in the individuals part.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Caution&lt;/strong&gt; You probably saw that in $var$coord explanation I was only interpreting variables that had good representation / long arrows / high cos2. The reason for this is that the projected angles between variables (those we see on the plane) cannot be blindly linked to original correlation coefficients / true angle between variables. For example we see that Altitude and p20 have a low &lt;strong&gt;projected&lt;/strong&gt; angle between them but p20 has a low cos2 meaning that the variable has a bad representation in the plane. Therefore one cannot conclude that this projected angle is a representative image of the real original space angle between the two variables. Therefore we cannot conclude that p20 and Altitude are necessarily correlated despite their low apparent projected angle. p20 having some of its inertia explained by another axis the projection limits us  : we cannot access the information whether this additional axis explaining the residual variance is correlated in the same manner to variables p20 and Altitude. We can only make a correlation assumption between variables according to their projected angle if the cos2 of both variables are fairly high, in that case a small projected angle correctly represent a small original angle hence correlation.&lt;/p&gt;

&lt;h5 id=&quot;varcontrib&quot;&gt;$var$contrib&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;						 Dim.1        Dim.2       Dim.3      Dim.4       Dim.5      Dim.6        Dim.7
Block.size.median        12.9277707 1.095245e+01  0.20543973  8.0327657  2.27049649 37.7151189 8.536027e+00
Altitude                 20.1996716 1.363651e+00 15.62687922  0.1000091  2.56474147  3.1078736 4.658538e+00
p20                       8.1715760 1.622204e+00 12.44435700 22.0549355  9.47272038  2.4759281 3.782411e+01
Wetness.index             5.1167544 2.588997e+01  7.32262274 12.7210962  0.03615206  4.3647009 5.417427e-04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;WIP&lt;/p&gt;
</content>
 </entry>
 

</feed>
