<!DOCTYPE html>
<html>

<head>
    <title>alelouis &mdash; cool stuff</title>
    <link href="https://fonts.googleapis.com/css?family=Barlow+Semi+Condensed" rel="stylesheet"> 
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
    </script>

    
        <title>Joint, marginal and conditional probabilities visualized</title>
    

    <meta name="description" content="cool stuff">

    

    <link rel="icon" href="/assets/img/blog.png">
    <link href="https://fonts.googleapis.com/css?family=Alegreya|Roboto|Roboto+Mono" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
</head>

<body>

    <div class="wrapper">
        <div class="header">
    <h1><a href="/">alelouis</a></h1>
    <ul>
        
        <li>
            <a href="/me">About</a>
        </li>
        
    </ul>
</div>

<div class="post">
	
    	
    <div class="post__title">
    	<h1>Joint, marginal and conditional probabilities visualized</h1>
    </div>
    <div class="post__date">
    	<p>December 5, 2017</p>
    </div>
    <div class="post__meta">
    	<p></p>
    </div>
    <div class="post__content"?>
        <h2 id="joint-marginal-and-conditional-probabilities-visualized">Joint, marginal and conditional probabilities visualized</h2>

<p>Being a visual type of person (if that’s actually a thing), I like thinking about concepts visually. It’s not rare to have many interpretations of the same concepts and each one chooses a mental model that suits us the best when learning new things.</p>

<p>I remember struggling to get my visual mental models right about probabiliy densities at first. While this is not typically the way it is teached, I find it easier to mentally understand and anticipate probability mechanics by visual means. I share here a graphical approach for theses concepts :</p>
<ul>
  <li>Joint probability density : <script type="math/tex">p(x, \theta)</script></li>
  <li>Marginal density : <script type="math/tex">p(x)</script></li>
  <li>Conditional density : <script type="math/tex">p(x\mid \theta)</script></li>
  <li>Likelihood : <script type="math/tex">L(\theta\mid x)</script></li>
</ul>

<p><span style="color:red">Warning : this is not supposed to be a rigorous approach to theses concepts, just intermediates mental models that can help some of you gaining intuitions about theses ideas.</span></p>

<h2 id="joint-probability-density">Joint probability density</h2>
<p>Joint distribution describre co-occurence of events. In terms of continuous distributions they are often noted as follow :</p>

<p><img style="margin: 0 auto; display: block; width : 100%;" src="../images/visual_prob/joint.svg" /></p>

<script type="math/tex; mode=display">p(x,\theta)</script>

<p>That is the distribution describing the joint probability of random variables <script type="math/tex">x</script> and <script type="math/tex">\theta</script>. For univariate RV <script type="math/tex">x</script> and <script type="math/tex">\theta</script>, the bivariate joint density <script type="math/tex">p(x,\theta)</script> can be represented as a surface (see fig.1). Integrating a joint distribution on the whole domains of definition of its variables gives one.</p>

<script type="math/tex; mode=display">\iint p(x,\theta)\;dxd\theta= 1</script>

<p>We can fix any of the random variables and see the behavior of the other variables by <strong>slicing</strong> the joint distribution (red and blue lines).
Note that <script type="math/tex">p(x=0,\theta)</script> and <script type="math/tex">p(x,\theta=0)</script> are not probability densities as they don’t integrate to 1.</p>

<p><img style="margin: 0 auto; display: block; width : 60%;" src="../images/visual_prob/joint_slices.svg" /></p>

<p>The whole shape of the curve is nonetheless interesting as we will see in conditional densities part.
If we were to integrate either of those curves it would give us the <strong>marginal</strong> probabilities of the fixed parameters at the fixed value. For instance :</p>

<script type="math/tex; mode=display">p(x=0) = \int p(x=0,\theta)\;d\theta</script>

<script type="math/tex; mode=display">p(\theta=0) = \int p(x,\theta=0)\;dx</script>

<p>We saw here a glimpse of what actually are marginal probabilities which are developed in the next section.</p>
<h2 id="marginal-density">Marginal density</h2>
<p>A marginal density represents the density of a unique random variable.</p>

<script type="math/tex; mode=display">p(x), p(\theta)</script>

<p>In case where the marginal is not specified, we can (in theory) fully recover it from the joint density. As noted earlier, integrating a slice of joint density at a given point equals the marginal at the very same point. The whole marginal density can be computed in a similar fashion. By taking many slices over a parameter and calculating the corresponding integral, we recover for each <script type="math/tex">x</script> value the corresponding area value / marginal value.</p>

<p><img style="margin: 0 auto; display: block; width : 100%;" src="../images/visual_prob/integration_marginal.svg" /></p>

<script type="math/tex; mode=display">p(x) = \int p(x,\theta)\;d\theta</script>

<p>On the above graph are drawn many <script type="math/tex">p(x=k,\theta)</script> which once integrated / marginalized w.r.t <script type="math/tex">\theta</script> equals <script type="math/tex">p(x=k)</script>. If we were to plot <script type="math/tex">p(x=k)</script> for infinitesimal <script type="math/tex">k</script> values we would obtain <script type="math/tex">p(x)</script>, similar operation for <script type="math/tex">p(\theta)</script> :</p>

<p><img style="margin: 0 auto; display: block; width : 60%;" src="../images/visual_prob/marginal.svg" /></p>

<p>Some properties appear obvious while reasoning geometrically :</p>
<ul>
  <li><script type="math/tex">p(x)</script> integrates to one as integrating over all the slices effectively integrates the whole joint density.</li>
  <li><script type="math/tex">p(x)</script> doesn’t depend on any other variable, as they have been <strong>integrated</strong> into the marginal.</li>
  <li><em>Caution</em> : <script type="math/tex">p(x)</script> computation seems straight forward geometrically but can actually be intractable analytically (and hard numerically too).</li>
</ul>

<h2 id="conditional-density">Conditional density</h2>

<p>Conditional densities will appear here very easy with what we already saw. Remember the slices we were taking inside the joint distributions, they weren’t really distributions because they didn’t integrated to one. Now if we normalize the slices by its areas, we obtain the conditional distributions of a variable <strong>given</strong> the other.</p>

<p><img style="margin: 0 auto; display: block; width : 60%;" src="../images/visual_prob/conditional.svg" /></p>

<p>For example, consider the slice <script type="math/tex">p(x,\theta=0)</script>, in order to make it integrate to one we normalize this function by its area (which is also the marginal taken at slice point that is to say <script type="math/tex">p(\theta=0)</script>, we then obtain <script type="math/tex">p(x\mid \theta=0)</script>. This is obviously generalizable to all <script type="math/tex">\theta</script> values. This actually gives us the definition of a conditional probability :</p>

<script type="math/tex; mode=display">p(x\mid \theta) = \frac{p(x,\theta)}{p(\theta)}</script>

<p>Notice how they have exactly the same shape as joint slices, the only difference being that they are normalized. We can wonder what looks like <script type="math/tex">p(x\mid \theta)</script> over all the different <script type="math/tex">\theta</script> :</p>

<p><img style="margin: 0 auto; display: block; width : 100%;" src="../images/visual_prob/pxmidtheta.svg" /></p>

<script type="math/tex; mode=display">p(x\mid \theta)</script>

<p>With a bit a training this type of result can easily become intuitional. All we did to obtain this surface (and not a real probability distribution as it does <em>not</em> integrates to one) from the joint is divide the whole joint distribution by the marginal of <script type="math/tex">\theta</script>. I represented it in <strong>orange</strong> on the graph so you can mentally understand how the marginal division affected the whole shape of the joint. From this surface each slice along the <script type="math/tex">\theta</script> axis represents the conditional distribution of <script type="math/tex">x</script> given the <script type="math/tex">\theta</script> slice point. But what does represent a slice along this <script type="math/tex">x</script> axis into this surface ?</p>

<h2 id="appendice--likelihood">Appendice : Likelihood</h2>

<p>The word likelihood is used in many contexts and often exchanged without precaution to mean probability. I will show you here how to visualize likelihood and how we can derive some properties of it. The previous surface we plotted represented the conditional distributions <script type="math/tex">p(x\mid \theta)</script> when considering fixed <script type="math/tex">\theta</script>. We obtained it by dividing the joint distribution by the marginal of <script type="math/tex">\theta</script>. Slicing along the <script type="math/tex">\theta</script> axis for different <script type="math/tex">\theta</script> values gives us many contional probabilities given the chosen <script type="math/tex">\theta</script>. Now if we were to consider cutting along the <script type="math/tex">x</script> axis, what would we get ? It is not a distribution by construction since we only normalized along the <script type="math/tex">\theta</script> axis. This slice represents the likelihood of <script type="math/tex">\theta</script> given <script type="math/tex">x</script>.</p>

<p><img style="margin: 0 auto; display: block; width : 100%;" src="../images/visual_prob/likelihood.svg" /></p>

<p>When considering <script type="math/tex">p(x\mid \theta)</script> visually, each slice direction represents respectively a conditional probability and a likelihood. The likelihood definition can be thought as two way to describe the same surface. For conditional probability we travel along the <script type="math/tex">x</script> axis fixing the <script type="math/tex">\theta</script> while for likelihood we travel along the <script type="math/tex">\theta</script> axis fixing the <script type="math/tex">x</script>.</p>

<script type="math/tex; mode=display">L(\theta\mid x) = p(x\mid \theta)</script>

<p><img style="margin: 0 auto; display: block; width : 60%;" src="../images/visual_prob/likeliarea.svg" /></p>

<p>A nice intuition you can take from this is that it is now <strong>obvious</strong> that likelihood is <strong>not</strong> a probability distribution (even if its confusing looking at the definition).</p>

<h2 id="conclusion">Conclusion</h2>

    </div>
</div>

<div class="footer">
    <p>&copy; 2017 <a href="http://alelouis.github.io">Alexis LOUIS</a></p>
</div>

    </div>

</body>

</html>
